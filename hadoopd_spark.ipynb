{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIyGtp0b7P4v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d33a5c3"
      },
      "source": [
        "# Task\n",
        "Demonstrate Hadoop and Spark in Google Colab by installing necessary dependencies, configuring the environment, initializing a Spark session, loading and processing a sample dataset, and extracting meaningful results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4808ec88"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries for PySpark and findspark in Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1250173d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires installing pyspark and findspark libraries. A single code block can be used to install both libraries using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bdd1b21",
        "outputId": "7bc153f6-fdcf-46cb-c2ac-78abb5900fb4"
      },
      "source": [
        "%pip install pyspark findspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22c3ae3c"
      },
      "source": [
        "## Configure environment\n",
        "\n",
        "### Subtask:\n",
        "Set up the environment variables for Spark and Java.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ad2008"
      },
      "source": [
        "**Reasoning**:\n",
        "Set up the environment variables for Spark and Java, and initialize findspark.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "d4459a53",
        "outputId": "e5a7a259-0ab3-45b5-bae8-149056702b6b"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.10/dist-packages/pyspark\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Unable to find py4j in /usr/local/lib/python3.10/dist-packages/pyspark/python, your SPARK_HOME may not be configured correctly",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1173071824.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SPARK_HOME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/usr/local/lib/python3.10/dist-packages/pyspark\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             raise Exception(\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[1;32m    163\u001b[0m                     \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Unable to find py4j in /usr/local/lib/python3.10/dist-packages/pyspark/python, your SPARK_HOME may not be configured correctly"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "154d52b1"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the `SPARK_HOME` environment variable was set incorrectly. The error message indicates that `py4j` could not be found in the specified path. I need to correct the `SPARK_HOME` path to the actual Spark installation directory, not the pyspark library directory. I will install Spark first and then set the `SPARK_HOME` environment variable to the correct path. I will also remove the unnecessary `findspark.init()` call as it is likely to fail again if the environment variable is not set correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-3Ij4gi7g_M",
        "outputId": "693231a4-7461-4da4-84b7-c405389882fc"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:4 https://cli.github.com/packages stable InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [80.2 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,942 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,783 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,560 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,209 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,579 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,357 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [42.7 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,272 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,305 kB]\n",
            "Fetched 35.2 MB in 5s (7,172 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "tar: spark-3.5.1-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbbe2c72"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed to extract the Spark archive because the file was not found. I need to correct the wget command to download the Spark archive to the correct location. I will keep the rest of the commands from the previous cell as they are still needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d7eb5c6",
        "outputId": "19683bd3-1bac-403a-b56a-20960765f9ba"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz -P /content/\n",
        "!tar xf /content/spark-3.5.1-bin-hadoop3.tgz -C /content/\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connected\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "tar: /content/spark-3.5.1-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2061237d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command still failed to extract the Spark archive because the wget command did not download the file. The output shows that the wget command did not complete successfully. I will try a different mirror for downloading Spark and also verify the download before attempting to extract it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27d9d674",
        "outputId": "25dd257a-3e03-42cf-a7c1-1b56ce37b8ee"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz -P /content/\n",
        "!ls /content/\n",
        "!tar xf /content/spark-3.5.1-bin-hadoop3.tgz -C /content/\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "sample_data  spark-3.5.1-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "002c681f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command successfully downloaded and extracted the Spark archive and set the SPARK_HOME environment variable. Now I need to set the JAVA_HOME environment variable and initialize findspark to make Spark available in the Python environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2aeb82d"
      },
      "source": [
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b427eea"
      },
      "source": [
        "## Initialize spark session\n",
        "\n",
        "### Subtask:\n",
        "Create a Spark session to start using Spark.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b0c4c1e"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the SparkSession class and create a Spark session builder, then get or create the Spark session.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0b8df31"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2d0fb30"
      },
      "source": [
        "## Load and process data\n",
        "\n",
        "### Subtask:\n",
        "Load a sample dataset (e.g., a CSV file) into a Spark DataFrame and perform some basic data processing (e.g., filtering, aggregation).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9233ad00"
      },
      "source": [
        "**Reasoning**:\n",
        "Download a sample CSV file, load it into a Spark DataFrame, perform a basic data processing operation, and display the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2285439",
        "outputId": "c50985ff-0738-4142-f202-34b1cbc674f3"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv -O gapminder.csv\n",
        "df = spark.read.csv(\"gapminder.csv\", header=True, inferSchema=True)\n",
        "filtered_df = df.filter(\"pop > 100000000\")\n",
        "filtered_df.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+------------+---------+--------+-----------+\n",
            "|   country|year|         pop|continent| lifeExp|  gdpPercap|\n",
            "+----------+----+------------+---------+--------+-----------+\n",
            "|Bangladesh|1987|1.03764241E8|     Asia|  52.819|751.9794035|\n",
            "|Bangladesh|1992|1.13704579E8|     Asia|  56.018|837.8101643|\n",
            "|Bangladesh|1997|1.23315288E8|     Asia|  59.412|972.7700352|\n",
            "|Bangladesh|2002| 1.3565679E8|     Asia|  62.013| 1136.39043|\n",
            "|Bangladesh|2007|1.50448339E8|     Asia|  64.062|1391.253792|\n",
            "|    Brazil|1972|1.00840058E8| Americas|  59.504|4985.711467|\n",
            "|    Brazil|1977|1.14313951E8| Americas|  61.489|6660.118654|\n",
            "|    Brazil|1982|1.28962939E8| Americas|  63.336|7030.835878|\n",
            "|    Brazil|1987|1.42938076E8| Americas|  65.205|7807.095818|\n",
            "|    Brazil|1992|1.55975974E8| Americas|  67.057|6950.283021|\n",
            "|    Brazil|1997|1.68546719E8| Americas|  69.388|7957.980824|\n",
            "|    Brazil|2002|1.79914212E8| Americas|  71.006|8131.212843|\n",
            "|    Brazil|2007|1.90010647E8| Americas|   72.39|9065.800825|\n",
            "|     China|1952|5.56263528E8|     Asia|    44.0|400.4486107|\n",
            "|     China|1957|   6.37408E8|     Asia|50.54896|575.9870009|\n",
            "|     China|1962|    6.6577E8|     Asia|44.50136|487.6740183|\n",
            "|     China|1967|    7.5455E8|     Asia|58.38112|612.7056934|\n",
            "|     China|1972|    8.6203E8|     Asia|63.11888|676.9000921|\n",
            "|     China|1977|   9.43455E8|     Asia|63.96736|741.2374699|\n",
            "|     China|1982|  1.000281E9|     Asia|  65.525|962.4213805|\n",
            "+----------+----+------------+---------+--------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10b671f3"
      },
      "source": [
        "## Extract meaningful results\n",
        "\n",
        "### Subtask:\n",
        "Analyze the processed data to extract insights or patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff41e36c"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate and display the average life expectancy and GDP per capita, the number of unique countries, and the average population by continent for the filtered data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2e52347",
        "outputId": "de8b3a01-c89b-447d-a4cd-96f4e2a0a7fa"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calculate and display average life expectancy and GDP per capita\n",
        "avg_life_expectancy = filtered_df.agg(F.avg(\"lifeExp\")).collect()[0][0]\n",
        "avg_gdp_per_capita = filtered_df.agg(F.avg(\"gdpPercap\")).collect()[0][0]\n",
        "print(f\"Average Life Expectancy: {avg_life_expectancy:.2f}\")\n",
        "print(f\"Average GDP per Capita: {avg_gdp_per_capita:.2f}\")\n",
        "\n",
        "# Calculate and display the number of unique countries\n",
        "unique_countries_count = filtered_df.select(\"country\").distinct().count()\n",
        "print(f\"Number of unique countries: {unique_countries_count}\")\n",
        "\n",
        "# Group by continent and calculate average population\n",
        "avg_pop_by_continent = filtered_df.groupBy(\"continent\").agg(F.avg(\"pop\").alias(\"average_population\"))\n",
        "avg_pop_by_continent.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Life Expectancy: 63.89\n",
            "Average GDP per Capita: 8637.62\n",
            "Number of unique countries: 10\n",
            "+---------+--------------------+\n",
            "|continent|  average_population|\n",
            "+---------+--------------------+\n",
            "|   Africa|1.2038009233333333E8|\n",
            "| Americas|1.8778264472727272E8|\n",
            "|     Asia|4.5764983661538464E8|\n",
            "+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac7b2354"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The average life expectancy within the filtered dataset (countries with population > 100,000,000) is approximately 63.89 years.\n",
        "*   The average GDP per capita in the filtered dataset is approximately \\$8637.62.\n",
        "*   The filtered dataset contains data for 10 unique countries.\n",
        "*   The average population by continent in the filtered data shows significant variation: Africa has an average population of approximately 1.20 x 10⁸, the Americas approximately 1.88 x 10⁸, and Asia approximately 4.58 x 10⁸.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The analysis highlights the impact of filtering on the dataset's statistical characteristics, providing insights into high-population countries.\n",
        "*   Further analysis could involve comparing these averages to the global averages or performing more granular analysis by country or year to identify trends.\n"
      ]
    }
  ]
}